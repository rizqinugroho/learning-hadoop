{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataAnalytic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNL9dfMMJ4I2bDP7w+lTog+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rizqinugroho/learning-hadoop/blob/main/learning-spark/DataAnalytic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark ditulis dalam bahasa Scala dan membutuhkan Java Virtual Machine (JVM) untuk bisa berjalan. Maka dari itu, kita membutuhkan java untuk dia bisa berjalan diatasnya"
      ],
      "metadata": {
        "id": "-4GTHWwj4sif"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xJKqm3WF3Nsw"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "3q04nTg23oGJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set the ‘environment’ path."
      ],
      "metadata": {
        "id": "PJcy6tPA488_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "b2f3vtLn4W0v"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu, kita. butuh menginstall **Findspark** library yang akan mencari spark dalam sistem dan akan menginstallnya sebagai regular library.  "
      ],
      "metadata": {
        "id": "3yRw7vd54_4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "\n",
        "#findspark.init({spark_home})\n",
        "findspark.init(\"/content/spark-3.2.1-bin-hadoop2.7/\")"
      ],
      "metadata": {
        "id": "oN_2UXEQ4aES"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sekarang, kita bisa import SparkSession dari pyspark.sql and membuat  SparkSession, yang merupakan pintu gerbang to Spark."
      ],
      "metadata": {
        "id": "1yzs642d5Hcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "SPBDMgLP4pjX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StringType**"
      ],
      "metadata": {
        "id": "Gow0jZ0MKKLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\n",
        "strType = StringType()\n",
        "strType = \"hello bro\"\n",
        "print(strType)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wstUSSh8KOJj",
        "outputId": "fbf745b0-3ad5-40ed-bea4-83664348d78d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello bro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Membuat PySpark ArrayType Column menggunakan StructType**"
      ],
      "metadata": {
        "id": "QKMba2ui9MRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
        " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
        " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
        "]\n",
        "\n",
        "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
        "schema = StructType([ \n",
        "    StructField(\"name\",StringType(),True), \n",
        "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
        "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
        "    StructField(\"currentState\", StringType(), True), \n",
        "    StructField(\"previousState\", StringType(), True)\n",
        "  ])\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5F_lVCU9SEM",
        "outputId": "e59015aa-8508-431c-c5a8-ea5b492dc384"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- languagesAtSchool: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- languagesAtWork: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- currentState: string (nullable = true)\n",
            " |-- previousState: string (nullable = true)\n",
            "\n",
            "+----------------+------------------+---------------+------------+-------------+\n",
            "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
            "+----------------+------------------+---------------+------------+-------------+\n",
            "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
            "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
            "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
            "+----------------+------------------+---------------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funsi Explode()**"
      ],
      "metadata": {
        "id": "OKe1Z5NCbxgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df.select(df.name,explode(df.languagesAtSchool)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jik1LioG9X4y",
        "outputId": "2d63ab9a-aea2-4b0d-d764-935282f8fff4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------+\n",
            "|            name|   col|\n",
            "+----------------+------+\n",
            "|    James,,Smith|  Java|\n",
            "|    James,,Smith| Scala|\n",
            "|    James,,Smith|   C++|\n",
            "|   Michael,Rose,| Spark|\n",
            "|   Michael,Rose,|  Java|\n",
            "|   Michael,Rose,|   C++|\n",
            "|Robert,,Williams|CSharp|\n",
            "|Robert,,Williams|    VB|\n",
            "+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split() function**"
      ],
      "metadata": {
        "id": "Q_FAYNi4b1W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split\n",
        "df.select(split(df.name,\",\").alias(\"nameAsArray\")).show()\n",
        "df.select(split(df.name,\",\").alias(\"nameAsArray\")).printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmk3JTF59ddV",
        "outputId": "335d14b9-6489-43e4-cfc7-5730bd791f3c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|         nameAsArray|\n",
            "+--------------------+\n",
            "|    [James, , Smith]|\n",
            "|   [Michael, Rose, ]|\n",
            "|[Robert, , Williams]|\n",
            "+--------------------+\n",
            "\n",
            "root\n",
            " |-- nameAsArray: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fungsi Array**"
      ],
      "metadata": {
        "id": "UocW6Xs0IbhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array\n",
        "df.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBQ0ptvy9fWT",
        "outputId": "e28a143f-a9bb-45c1-a1df-9243e6cb617e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------+\n",
            "|            name|  States|\n",
            "+----------------+--------+\n",
            "|    James,,Smith|[OH, CA]|\n",
            "|   Michael,Rose,|[NY, NJ]|\n",
            "|Robert,,Williams|[UT, NV]|\n",
            "+----------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Array Contains**"
      ],
      "metadata": {
        "id": "0X7WZIiwIzge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array_contains\n",
        "df.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n",
        "    .alias(\"array_contains\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NycTr6Ua9qSm",
        "outputId": "dd7095b8-ea55-4fe5-8e9d-bde0d9730bb2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------------+\n",
            "|            name|array_contains|\n",
            "+----------------+--------------+\n",
            "|    James,,Smith|          true|\n",
            "|   Michael,Rose,|          true|\n",
            "|Robert,,Williams|         false|\n",
            "+----------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"title\", \"price\", \"year_written\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9onSEAb9ygT",
        "outputId": "c994c060-78e3-4a69-fd85-22ed498ad470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----+------------+\n",
            "|           title|price|year_written|\n",
            "+----------------+-----+------------+\n",
            "|Northanger Abbey| 18.2|        1814|\n",
            "|   War and Peace| 12.7|        1865|\n",
            "|   Anna Karenina| 13.5|        1875|\n",
            "|   Mrs. Dalloway| 25.0|        1925|\n",
            "|       The Hours|12.35|        1999|\n",
            "+----------------+-----+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MapType()**"
      ],
      "metadata": {
        "id": "lulVG5RaYMjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
        "schema = StructType([\n",
        "    StructField('name', StringType(), True),\n",
        "    StructField('properties', MapType(StringType(),StringType()),True)\n",
        "])\n",
        "\n",
        "dataDictionary = [\n",
        "        ('James',{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',{'hair':'brown','eye':None}),\n",
        "        ('Robert',{'hair':'red','eye':'black'}),\n",
        "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
        "        ('Jefferson',{'hair':'brown','eye':''})\n",
        "        ]\n",
        "df = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtRjzv56-obT",
        "outputId": "0de2bd43-1ac1-47fa-9cca-6d6410f3a89e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+-----------------------------+\n",
            "|name      |properties                   |\n",
            "+----------+-----------------------------+\n",
            "|James     |{eye -> brown, hair -> black}|\n",
            "|Michael   |{eye -> null, hair -> brown} |\n",
            "|Robert    |{eye -> black, hair -> red}  |\n",
            "|Washington|{eye -> grey, hair -> grey}  |\n",
            "|Jefferson |{eye -> , hair -> brown}     |\n",
            "+----------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Akses Element Map Type**"
      ],
      "metadata": {
        "id": "p_SXWHfybci-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3=df.rdd.map(lambda x: \\\n",
        "    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n",
        "    .toDF([\"name\",\"hair\",\"eye\"])\n",
        "df3.printSchema()\n",
        "df3.show()"
      ],
      "metadata": {
        "id": "WTlh9gToTBkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1e9873-f268-4754-8fba-4790d4059e1a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- hair: string (nullable = true)\n",
            " |-- eye: string (nullable = true)\n",
            "\n",
            "+----------+-----+-----+\n",
            "|      name| hair|  eye|\n",
            "+----------+-----+-----+\n",
            "|     James|black|brown|\n",
            "|   Michael|brown| null|\n",
            "|    Robert|  red|black|\n",
            "|Washington| grey| grey|\n",
            "| Jefferson|brown|     |\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mari kita gunakan cara lain untuk mendapatkan nilai kunci dari Peta menggunakan getItem()tipe Column, metode ini mengambil kunci sebagai argumen dan mengembalikan nilai."
      ],
      "metadata": {
        "id": "98G2iSKrblKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n",
        "  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n",
        "  .drop(\"properties\") \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMi93uGFTXjA",
        "outputId": "db9ac54d-965e-4d40-ff1f-cf098993a1a4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+\n",
            "|      name| hair|  eye|\n",
            "+----------+-----+-----+\n",
            "|     James|black|brown|\n",
            "|   Michael|brown| null|\n",
            "|    Robert|  red|black|\n",
            "|Washington| grey| grey|\n",
            "| Jefferson|brown|     |\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**maptype explode()**"
      ],
      "metadata": {
        "id": "dZYNCy5MeUuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df.select(df.name,explode(df.properties)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLq7AoaseY-F",
        "outputId": "3bfde178-6c40-44eb-e3e8-7981d1839184"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-----+\n",
            "|      name| key|value|\n",
            "+----------+----+-----+\n",
            "|     James| eye|brown|\n",
            "|     James|hair|black|\n",
            "|   Michael| eye| null|\n",
            "|   Michael|hair|brown|\n",
            "|    Robert| eye|black|\n",
            "|    Robert|hair|  red|\n",
            "|Washington| eye| grey|\n",
            "|Washington|hair| grey|\n",
            "| Jefferson| eye|     |\n",
            "| Jefferson|hair|brown|\n",
            "+----------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**map_keys()**"
      ],
      "metadata": {
        "id": "hEmDX73demeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import map_keys\n",
        "df.select(df.name,map_keys(df.properties)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRoiYRexetQ4",
        "outputId": "d7ff4063-a1c5-4c17-b6ff-df016dbf8634"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+\n",
            "|      name|map_keys(properties)|\n",
            "+----------+--------------------+\n",
            "|     James|         [eye, hair]|\n",
            "|   Michael|         [eye, hair]|\n",
            "|    Robert|         [eye, hair]|\n",
            "|Washington|         [eye, hair]|\n",
            "| Jefferson|         [eye, hair]|\n",
            "+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**map_values()**"
      ],
      "metadata": {
        "id": "_vPey0bIPCSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import map_values\n",
        "df.select(df.name,map_values(df.properties)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3cIrHdUPE-f",
        "outputId": "ba7e66a7-0c30-4158-e587-68a1f349d767"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------------+\n",
            "|      name|map_values(properties)|\n",
            "+----------+----------------------+\n",
            "|     James|        [brown, black]|\n",
            "|   Michael|         [null, brown]|\n",
            "|    Robert|          [black, red]|\n",
            "|Washington|          [grey, grey]|\n",
            "| Jefferson|             [, brown]|\n",
            "+----------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**datetype()**"
      ],
      "metadata": {
        "id": "acRZHrXqQdNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DateType\n",
        "\n",
        "\n",
        "\n",
        "# Creation of a dummy dataframe:\n",
        "df1 = spark.createDataFrame([(\"1991-11-20\",\"1991-11-20\",\"1991-11-20\"), \n",
        "                            (\"1992-11-20\",\"1992-11-20\",\"1993-11-20\")], schema=['first', 'second', 'third'])\n",
        "\n",
        "#using cast to convert from string to datetype\n",
        "df = df1.withColumn('test', col('first').cast(DateType()))\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMTV-6EuQg4J",
        "outputId": "cfe3420a-6482-4c7d-f538-d588c302fdb2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+----------+\n",
            "|     first|    second|     third|      test|\n",
            "+----------+----------+----------+----------+\n",
            "|1991-11-20|1991-11-20|1991-11-20|1991-11-20|\n",
            "|1992-11-20|1992-11-20|1993-11-20|1992-11-20|\n",
            "+----------+----------+----------+----------+\n",
            "\n",
            "root\n",
            " |-- first: string (nullable = true)\n",
            " |-- second: string (nullable = true)\n",
            " |-- third: string (nullable = true)\n",
            " |-- test: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qp4AbJluUUd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Timestamp**"
      ],
      "metadata": {
        "id": "5SFPavuTURqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "df = spark.createDataFrame(\n",
        "    [(\"1997-02-28 10:30:00\",), (\"1997-02-28 10:33:00\",), (\"1997-02-28 10:35:00\",)], \n",
        "    ['date_str'])\n",
        "\n",
        "converted_df = df.select(to_timestamp(df.date_str, 'yyyy-MM-dd HH:mm:ss')).alias('dt_col')\n",
        "\n",
        "\n",
        "converted_df.show()\n",
        "converted_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gPepIKTUV7O",
        "outputId": "a19ab48c-121c-475a-934b-d1d204dff3cb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------+\n",
            "|to_timestamp(date_str, yyyy-MM-dd HH:mm:ss)|\n",
            "+-------------------------------------------+\n",
            "|                        1997-02-28 10:30:00|\n",
            "|                        1997-02-28 10:33:00|\n",
            "|                        1997-02-28 10:35:00|\n",
            "+-------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- to_timestamp(date_str, yyyy-MM-dd HH:mm:ss): timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boolean**"
      ],
      "metadata": {
        "id": "yZgaWuksYEjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list  of students  data\n",
        "data = [[\"1\", \"Ryan\", \"DU\"],\n",
        "        [\"2\", \"Fina\", \"DU\"],\n",
        "        [\"3\", \"Imam\", \"BHU\"],\n",
        "        [\"4\", \"Bagas\", \"LPU\"],\n",
        "        [\"1\", \"Budi\", \"KLMP\"],\n",
        "        [\"5\", \"Ani\", \"IIT\"]]\n",
        "  \n",
        "# specify column names\n",
        "columns = ['student_ID', 'student_NAME', 'college']\n",
        "  \n",
        "# creating a dataframe from the lists of data\n",
        "dataframe = spark.createDataFrame(data, columns)\n",
        "\n",
        "#and using &\n",
        "dataframe.filter((dataframe.college == \"DU\") &\n",
        "                 (dataframe.student_ID == \"1\")).show()\n",
        "#or using |\n",
        "dataframe.filter((dataframe.college == \"DU\") |\n",
        "                 (dataframe.student_ID == \"1\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf5oKSjgYECV",
        "outputId": "0c30808a-c207-401c-d5e2-2b42391dcb0a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------+\n",
            "|student_ID|student_NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|        Ryan|     DU|\n",
            "+----------+------------+-------+\n",
            "\n",
            "+----------+------------+-------+\n",
            "|student_ID|student_NAME|college|\n",
            "+----------+------------+-------+\n",
            "|         1|        Ryan|     DU|\n",
            "|         2|        Fina|     DU|\n",
            "|         1|        Budi|   KLMP|\n",
            "+----------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number**"
      ],
      "metadata": {
        "id": "KorlrExWaQOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(\"A\", 20), (\"B\", 30), (\"D\", 80)],[\"Letter\", \"Number\"])\n",
        "sum_value = df.groupBy().sum()\n",
        "\n",
        "sum_value.show()\n",
        "sum_value.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loI1dx6BaYPZ",
        "outputId": "bc48044f-4f67-40c5-9bdd-62c604c4d733"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|sum(Number)|\n",
            "+-----------+\n",
            "|        130|\n",
            "+-----------+\n",
            "\n",
            "root\n",
            " |-- sum(Number): long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aggregate - Group By**"
      ],
      "metadata": {
        "id": "9AmsXVeOce8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "\n",
        "#group by\n",
        "df_group_by = df.groupby('department')\n",
        "\n",
        "#max salary\n",
        "print(\"====Max salary ===\")\n",
        "df_group_by.max().show()\n",
        "\n",
        "#sum salary\n",
        "print(\"====sum salary ===\")\n",
        "df_group_by.sum().show()\n",
        "\n",
        "\n",
        "#avg salary\n",
        "print(\"====avg salary ===\")\n",
        "df_group_by.avg().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi6c_4-eciQJ",
        "outputId": "781aef2c-579e-408f-ff3e-b2b7fdbaa89d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====Max salary ===\n",
            "+----------+-----------+\n",
            "|department|max(salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       4600|\n",
            "|   Finance|       3900|\n",
            "| Marketing|       3000|\n",
            "+----------+-----------+\n",
            "\n",
            "====sum salary ===\n",
            "+----------+-----------+\n",
            "|department|sum(salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|      18800|\n",
            "|   Finance|      10200|\n",
            "| Marketing|       5000|\n",
            "+----------+-----------+\n",
            "\n",
            "====avg salary ===\n",
            "+----------+-----------+\n",
            "|department|avg(salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3760.0|\n",
            "|   Finance|     3400.0|\n",
            "| Marketing|     2500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**JOIN Statement**"
      ],
      "metadata": {
        "id": "PUPzDqkMgzWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "  ]\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
        "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)\n",
        "\n",
        "#Inner Join \n",
        "print(\"===Inner Join Example ===\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
        "     .show(truncate=False)\n",
        "\n",
        "#Full Outer Join\n",
        "print(\"===Full Outer Join Example ===\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "#Left Outer Join\n",
        "print(\"===Left Outer Join Example ===\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show(truncate=False)\n",
        "\n",
        "#Right Outer Join\n",
        "print(\"===Right Outer Join Example ===\")\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\").show(truncate=False)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr1pEl95g4Tv",
        "outputId": "9cdb5d52-129b-428d-ec38-d9d487f8da1b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- superior_emp_id: long (nullable = true)\n",
            " |-- year_joined: string (nullable = true)\n",
            " |-- emp_dept_id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n",
            "===Inner Join Example ===\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "===Full Outer Join Example ===\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "===Left Outer Join Example ===\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "===Right Outer Join Example ===\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}